{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82368df4",
   "metadata": {},
   "source": [
    "# HiddenOracle3 with Result Saving\n",
    "\n",
    "This notebook extends the functionality of the previous version by saving the evaluation results to a JSON file. That way, you keep a permanent record of your local model's answers, factuality, and explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277a99eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# ---------------------------------------\n",
    "# Configuration Variables\n",
    "# ---------------------------------------\n",
    "\n",
    "# (1) OpenAI API Key\n",
    "OPENAI_API_KEY = \"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\"\n",
    "\n",
    "# (2) Paths\n",
    "# Make sure these are valid paths in your Google Drive.\n",
    "DATASET_PATH = \"/content/drive/My Drive/Colab Notebooks/HiddenOracle3/data.json\"  # Example path\n",
    "LOCAL_MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"  # Replace if needed\n",
    "\n",
    "# Path to where we'll write the JSON results.\n",
    "OUTPUT_JSON_PATH = \"/content/drive/My Drive/Colab Notebooks/HiddenOracle3/evaluation_results.json\"\n",
    "\n",
    "# ---------------------------------------\n",
    "# OpenAI Setup\n",
    "# ---------------------------------------\n",
    "def configure_openai() -> None:\n",
    "    \"\"\"\n",
    "    Sets the OpenAI API key.\n",
    "    Call this once before using any OpenAI endpoints.\n",
    "    \"\"\"\n",
    "    openai.api_key = OPENAI_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceed8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAI:\n",
    "    \"\"\"A simple wrapper class for OpenAI's ChatCompletion API calls.\"\"\"\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        openai.api_key = self.api_key\n",
    "\n",
    "    @property\n",
    "    def chat(self):\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def completions(self):\n",
    "        return self\n",
    "\n",
    "    def create(\n",
    "        self,\n",
    "        model: str,\n",
    "        messages: List[Dict[str, Any]],\n",
    "        tools: List[Dict[str, Any]] = None,\n",
    "        tool_choice: Dict[str, Any] = None,\n",
    "        **kwargs\n",
    "    ) -> Any:\n",
    "        \"\"\"\n",
    "        Mocked method to illustrate function-calling usage. In a real scenario, you would\n",
    "        call `openai.ChatCompletion.create(...)` with the appropriate function definitions.\n",
    "        \"\"\"\n",
    "\n",
    "        user_message = messages[-1][\"content\"].lower()\n",
    "\n",
    "        # Simple example logic to simulate an API response.\n",
    "        is_factual = False\n",
    "        explanation = \"Local LLM answer does not match known correct answers.\"\n",
    "\n",
    "        if \"example_correct_keyword\" in user_message:\n",
    "            is_factual = True\n",
    "            explanation = \"The local LLM's answer matches the known correct answer.\"\n",
    "\n",
    "        # Mock a function call structure (simulating GPT-4 function calling)\n",
    "        response = {\n",
    "            \"choices\": [\n",
    "                {\n",
    "                    \"message\": {\n",
    "                        \"tool_calls\": [\n",
    "                            {\n",
    "                                \"function\": {\n",
    "                                    \"name\": \"evaluate_answer\",\n",
    "                                    \"arguments\": json.dumps({\n",
    "                                        \"is_factual\": is_factual,\n",
    "                                        \"explanation\": explanation\n",
    "                                    })\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aad4aee",
   "metadata": {},
   "source": [
    "## Load Dataset and Local Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76d7338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Loads the dataset from a JSON file.\n",
    "    \"\"\"\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def get_local_llm_answer(question: str, generation_pipeline) -> str:\n",
    "    \"\"\"\n",
    "    Uses a text-generation pipeline to get an answer from the local LLM.\n",
    "    \"\"\"\n",
    "    results = generation_pipeline(\n",
    "        question, max_new_tokens=25, num_return_sequences=1, do_sample=False\n",
    "    )\n",
    "    generated_text = results[0][\"generated_text\"]\n",
    "\n",
    "    # Optionally trim out the prompt if it's included in the output\n",
    "    if generated_text.startswith(question):\n",
    "        answer = generated_text[len(question):].strip()\n",
    "    else:\n",
    "        answer = generated_text.strip()\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad6789f",
   "metadata": {},
   "source": [
    "## Evaluate correctness with the new function-calling approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac2838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_openai_api(\n",
    "    question: str, local_llm_answer: str, correct_answers: List[str]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calls the OpenAI ChatCompletion with the new function-calling style to evaluate\n",
    "    the local LLM's response.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"evaluate_answer\",\n",
    "                \"description\": \"Evaluate the correctness of a local LLM's answer.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"is_factual\": {\n",
    "                            \"type\": \"boolean\",\n",
    "                            \"description\": \"True if the local LLM answer is correct.\"\n",
    "                        },\n",
    "                        \"explanation\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Explanation of why the answer is correct or incorrect.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"is_factual\", \"explanation\"],\n",
    "                    \"additionalProperties\": False\n",
    "                },\n",
    "                \"strict\": True\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a judge. Compare the local LLM answer with known correct answers.\\n\"\n",
    "                \"Use the evaluate_answer function to return factuality and reasoning.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"Question: {question}\\n\"\n",
    "                f\"Local LLM Answer: {local_llm_answer}\\n\"\n",
    "                f\"Known Correct Answers: {correct_answers}\"\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"evaluate_answer\"}},\n",
    "    )\n",
    "\n",
    "    if completion[\"choices\"][0][\"message\"].get(\"tool_calls\"):\n",
    "        tool_call = completion[\"choices\"][0][\"message\"][\"tool_calls\"][0]\n",
    "        arguments = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "        return arguments\n",
    "    else:\n",
    "        return {\n",
    "            \"is_factual\": False,\n",
    "            \"explanation\": \"No tool call was made.\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563aae96",
   "metadata": {},
   "source": [
    "## Main Evaluation Routine\n",
    "Here, in addition to printing the results, we will accumulate them in a list (`evaluation_results`) and **write** them out to `OUTPUT_JSON_PATH` at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135c5337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dataset_path: str, local_model_name: str, output_json_path: str) -> None:\n",
    "    \"\"\"\n",
    "    1. Loads the dataset.\n",
    "    2. Initializes the local model.\n",
    "    3. Generates an answer with the local LLM.\n",
    "    4. Uses either a simple string match or OpenAI's function calling for deeper checks.\n",
    "    5. Prints the results and saves them to a JSON file.\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(dataset_path)\n",
    "\n",
    "    # Initialize local model pipeline\n",
    "    tokenizer = AutoTokenizer.from_pretrained(local_model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        local_model_name,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # List to store results for writing to file\n",
    "    evaluation_results = []\n",
    "\n",
    "    # Evaluate each dataset item\n",
    "    for item in dataset:\n",
    "        question = item[\"question\"]\n",
    "        correct_answers = item[\"answers\"]\n",
    "\n",
    "        # 1) Generate answer from local LLM\n",
    "        local_answer = get_local_llm_answer(question, generation_pipeline)\n",
    "\n",
    "        # 2) Simple string match check\n",
    "        matched = any(\n",
    "            ans.lower() in local_answer.lower() for ans in correct_answers\n",
    "        )\n",
    "\n",
    "        # 3) If we have a match, skip the OpenAI API and record success.\n",
    "        if matched:\n",
    "            result = {\n",
    "                \"is_factual\": True,\n",
    "                \"explanation\": \"String match with known correct answers; no OpenAI call needed.\",\n",
    "            }\n",
    "        else:\n",
    "            # 4) If no match, call the OpenAI function-calling API\n",
    "            result = evaluate_with_openai_api(\n",
    "                question=question,\n",
    "                local_llm_answer=local_answer,\n",
    "                correct_answers=correct_answers,\n",
    "            )\n",
    "\n",
    "        # 5) Print results\n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Local LLM Answer: {local_answer}\")\n",
    "        print(f\"Is Factual: {result['is_factual']}\")\n",
    "        print(f\"Explanation: {result['explanation']}\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "\n",
    "        # 6) Accumulate results for saving\n",
    "        evaluation_results.append({\n",
    "            \"question\": question,\n",
    "            \"answers\": correct_answers,\n",
    "            \"local_llm_answer\": local_answer,\n",
    "            \"is_factual\": result['is_factual'],\n",
    "            \"explanation\": result['explanation']\n",
    "        })\n",
    "\n",
    "    # 7) Write results to a JSON file (e.g., in Google Drive)\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(evaluation_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\nAll results saved to: {output_json_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc1c088",
   "metadata": {},
   "source": [
    "## Running the Evaluation\n",
    "\n",
    "In **Colab**, mount your Google Drive:\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "```\n",
    "Make sure the notebook and data are in valid paths under `/content/drive/...`. Then you can run the evaluation cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a24b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    configure_openai()\n",
    "    evaluate_model(\n",
    "        dataset_path=DATASET_PATH,\n",
    "        local_model_name=LOCAL_MODEL_NAME,\n",
    "        output_json_path=OUTPUT_JSON_PATH\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9"
  },
  "name": "HiddenOracle3"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

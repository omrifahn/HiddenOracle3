{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HiddenOracle3\n",
    "### A Combined Jupyter Notebook Version\n",
    "\n",
    "This notebook combines the functionality of the original **`config.py`** and **`main.py`** files into a single Jupyter Notebook. It will:\n",
    "1. Define the OpenAI configuration and global variables.\n",
    "2. Load a dataset from a JSON file.\n",
    "3. Initialize a local LLM using Hugging Face Transformers.\n",
    "4. Generate answers using the local LLM.\n",
    "5. Evaluate correctness using either a simple string match or an OpenAI function-calling interface.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/omrifahn/anaconda3/envs/HiddenOracle3-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# ---------------------------------------\n",
    "# Configuration Variables\n",
    "# ---------------------------------------\n",
    "\n",
    "# (1) OpenAI API Key\n",
    "OPENAI_API_KEY = \"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\"\n",
    "\n",
    "# (2) Paths\n",
    "DATASET_PATH = \"./data/data.json\"\n",
    "\n",
    "# (3) Local model name\n",
    "LOCAL_MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# ---------------------------------------\n",
    "# OpenAI Setup\n",
    "# ---------------------------------------\n",
    "def configure_openai() -> None:\n",
    "    \"\"\"\n",
    "    Sets the OpenAI API key.\n",
    "    Call this once before using any OpenAI endpoints.\n",
    "    \"\"\"\n",
    "    openai.api_key = OPENAI_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "class OpenAI:\n",
    "    \"\"\"A simple wrapper class for OpenAI's ChatCompletion API calls.\"\"\"\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        openai.api_key = self.api_key\n",
    "\n",
    "    @property\n",
    "    def chat(self):\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def completions(self):\n",
    "        return self\n",
    "\n",
    "    def create(\n",
    "        self,\n",
    "        model: str,\n",
    "        messages: List[Dict[str, Any]],\n",
    "        tools: List[Dict[str, Any]] = None,\n",
    "        tool_choice: Dict[str, Any] = None,\n",
    "        **kwargs\n",
    "    ) -> Any:\n",
    "        \"\"\"\n",
    "        Mocked method to illustrate function-calling usage. In a real scenario, you would\n",
    "        call something like `openai.ChatCompletion.create(...)` with the appropriate function\n",
    "        definitions.\n",
    "\n",
    "        This is a simplified placeholder that returns a mock structure consistent with\n",
    "        how function-calling responses might be.\n",
    "        \"\"\"\n",
    "        # This part would typically involve sending function calling parameters to OpenAI.\n",
    "        # We'll assume we get a structured response back. We'll mimic the tool call here.\n",
    "\n",
    "        # Extract content from user prompt to do a trivial check.\n",
    "        user_message = messages[-1][\"content\"].lower()\n",
    "        # If it looks like the local LLM answer is correct, we mock a success.\n",
    "\n",
    "        is_factual = False\n",
    "        explanation = \"The answer does not match any known correct answers.\"\n",
    "\n",
    "        # You can fine-tune this logic or make an actual call to GPT.\n",
    "        if \"apple\" in user_message:\n",
    "            is_factual = True\n",
    "            explanation = \"The local LLM's answer matches: Apple is indeed correct.\"\n",
    "\n",
    "        # Mock a function call structure (simulating GPT-4 function calling)\n",
    "        response = {\n",
    "            \"choices\": [\n",
    "                {\n",
    "                    \"message\": {\n",
    "                        \"tool_calls\": [\n",
    "                            {\n",
    "                                \"function\": {\n",
    "                                    \"name\": \"evaluate_answer\",\n",
    "                                    \"arguments\": json.dumps({\n",
    "                                        \"is_factual\": is_factual,\n",
    "                                        \"explanation\": explanation\n",
    "                                    })\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset and Local Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Loads the dataset from a JSON file.\n",
    "    \"\"\"\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def get_local_llm_answer(question: str, generation_pipeline) -> str:\n",
    "    \"\"\"\n",
    "    Uses a text-generation pipeline to get an answer from the local LLM.\n",
    "    \"\"\"\n",
    "    results = generation_pipeline(\n",
    "        question, max_new_tokens=25, num_return_sequences=1, do_sample=False\n",
    "    )\n",
    "    generated_text = results[0][\"generated_text\"]\n",
    "\n",
    "    # Optionally trim out the prompt if it's included in the output\n",
    "    if generated_text.startswith(question):\n",
    "        answer = generated_text[len(question) :].strip()\n",
    "    else:\n",
    "        answer = generated_text.strip()\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate correctness with the NEW function calling (tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_openai_api(\n",
    "    question: str, local_llm_answer: str, correct_answers: List[str]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calls the OpenAI ChatCompletion with the new function-calling style to evaluate\n",
    "    the local LLM's response.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"evaluate_answer\",\n",
    "                \"description\": \"Evaluate the correctness of a local LLM's answer.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"is_factual\": {\n",
    "                            \"type\": \"boolean\",\n",
    "                            \"description\": \"True if the local LLM answer is correct.\"\n",
    "                        },\n",
    "                        \"explanation\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Explanation of why the answer is correct or incorrect.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"is_factual\", \"explanation\"],\n",
    "                    \"additionalProperties\": False\n",
    "                },\n",
    "                \"strict\": True\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a judge. Compare the local LLM answer with known correct answers. \"\n",
    "                \"Use the evaluate_answer function to return factuality and reasoning.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"Question: {question}\\n\"\n",
    "                f\"Local LLM Answer: {local_llm_answer}\\n\"\n",
    "                f\"Known Correct Answers: {correct_answers}\"\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"evaluate_answer\"}},\n",
    "    )\n",
    "\n",
    "    if completion[\"choices\"][0][\"message\"].get(\"tool_calls\"):\n",
    "        tool_call = completion[\"choices\"][0][\"message\"][\"tool_calls\"][0]\n",
    "        arguments = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "        return arguments\n",
    "    else:\n",
    "        return {\"is_factual\": False, \"explanation\": \"No tool call was made.\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Evaluation Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dataset_path: str, local_model_name: str) -> None:\n",
    "    \"\"\"\n",
    "    1. Loads the dataset.\n",
    "    2. Initializes the local model.\n",
    "    3. Generates an answer with the local LLM.\n",
    "    4. Uses either:\n",
    "       - Simple string matching to determine correctness, or\n",
    "       - OpenAI's function calling for deeper evaluation (if no match found).\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(dataset_path)\n",
    "\n",
    "    # Initialize local model pipeline\n",
    "    tokenizer = AutoTokenizer.from_pretrained(local_model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        local_model_name,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Evaluate each dataset item\n",
    "    for item in dataset:\n",
    "        question = item[\"question\"]\n",
    "        correct_answers = item[\"answers\"]\n",
    "\n",
    "        # 1) Generate answer from local LLM\n",
    "        local_answer = get_local_llm_answer(question, generation_pipeline)\n",
    "\n",
    "        # 2) Simple string match check\n",
    "        matched = any(\n",
    "            ans.lower() in local_answer.lower() for ans in correct_answers\n",
    "        )\n",
    "\n",
    "        # 3) If we have a match, skip calling the OpenAI API and record success.\n",
    "        if matched:\n",
    "            result = {\n",
    "                \"is_factual\": True,\n",
    "                \"explanation\": \"String match with known correct answers; no OpenAI call needed.\",\n",
    "            }\n",
    "        else:\n",
    "            # 4) If no match, call the OpenAI function-calling API for deeper check.\n",
    "            result = evaluate_with_openai_api(\n",
    "                question=question,\n",
    "                local_llm_answer=local_answer,\n",
    "                correct_answers=correct_answers,\n",
    "            )\n",
    "\n",
    "        # 5) Print results\n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Local LLM Answer: {local_answer}\")\n",
    "        print(f\"Is Factual: {result['is_factual']}\")\n",
    "        print(f\"Explanation: {result['explanation']}\")\n",
    "        print(\"--------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "Once the above cells have been run, simply call the `evaluate_model` function below to initiate the process. Make sure to provide a proper `data.json` in your `DATASET_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Configure OpenAI\n",
    "    configure_openai()\n",
    "\n",
    "    # Run evaluation\n",
    "    evaluate_model(DATASET_PATH, LOCAL_MODEL_NAME)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HiddenOracle3-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
